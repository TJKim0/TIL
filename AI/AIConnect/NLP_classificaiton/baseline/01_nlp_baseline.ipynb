{"cells":[{"cell_type":"markdown","id":"68696f7d-99f7-4a1b-97b1-882d0c5565e9","metadata":{"id":"68696f7d-99f7-4a1b-97b1-882d0c5565e9"},"source":["# [모의 캐글 - 게임] 비매너 댓글 식별 \n","\n","- 자연어 multi label classification 과제\n","- 작성자 : MNC Sukyung Kim (skkim@mnc.ai)\n","\n","참고 논문 : \n","- [BERT: Pre-training of Deep Bidirectional Transformers for\n","Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n","- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaRNu3hOag0z","executionInfo":{"status":"ok","timestamp":1645583425063,"user_tz":-540,"elapsed":17224,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}},"outputId":"6148b514-85fd-45c8-ffd5-c3b363646638"},"id":"SaRNu3hOag0z","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"0bfa42a1-848b-4d3b-b13e-b65cf0972404","metadata":{"id":"0bfa42a1-848b-4d3b-b13e-b65cf0972404"},"source":["# 1. 환경 설정 및 라이브러리 불러오기"]},{"cell_type":"code","execution_count":4,"id":"e6c0e90b-6f4c-45fd-b83c-140dcde11d2b","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"e6c0e90b-6f4c-45fd-b83c-140dcde11d2b","executionInfo":{"status":"ok","timestamp":1645583456279,"user_tz":-540,"elapsed":8092,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}},"outputId":"1d2f3156-d87b-4a49-9d37-c91bcb062b67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting attrdict\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 2)) (1.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 3)) (1.3.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 4)) (1.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 5)) (3.2.2)\n","Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (7.6.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 3)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 3)) (1.21.5)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 4)) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 5)) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 5)) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 5)) (1.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (2019.12.20)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 70.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (4.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 22.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.3 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 63.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (3.10.0.2)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.1.1)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.1.3)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.2.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (1.0.2)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.5.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.10.1)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (3.5.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.3.5)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.1.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.7.5)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.9.2)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.3.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (21.4.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.18.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (3.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (5.6.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.13.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (2.11.3)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (22.3.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (2.0.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.5.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (1.5.0)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (4.1.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.8.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 7)) (0.5.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt (line 6)) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, attrdict\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed attrdict-2.0.1 huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"]}],"source":["!pip install -r /content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/requirements.txt"]},{"cell_type":"code","execution_count":5,"id":"ff15bef2-c669-4cb1-bf76-256154c4858b","metadata":{"id":"ff15bef2-c669-4cb1-bf76-256154c4858b","executionInfo":{"status":"ok","timestamp":1645583469511,"user_tz":-540,"elapsed":8851,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","import json\n","import numpy as np\n","import shutil\n","\n","from sklearn.metrics import f1_score\n","from datetime import datetime, timezone, timedelta\n","import random\n","from tqdm import tqdm\n","\n","\n","from attrdict import AttrDict\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils import *\n","from torch.optim import Adam, AdamW\n","\n","from transformers import logging, get_linear_schedule_with_warmup\n","\n","\n","from transformers import ( \n","    BertConfig,\n","    ElectraConfig,\n","    ElectraConfig\n",")\n","\n","from transformers import (\n","    BertTokenizer,  \n","    AutoTokenizer,\n","    ElectraTokenizer,\n",")\n","\n","from transformers import (\n","    BertModel,\n","    AutoModel, \n","    ElectraForSequenceClassification,\n","    BertForSequenceClassification\n",")\n"]},{"cell_type":"code","execution_count":6,"id":"5c95b317-b37f-4c59-8eaf-25ce048eb753","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5c95b317-b37f-4c59-8eaf-25ce048eb753","executionInfo":{"status":"ok","timestamp":1645583469512,"user_tz":-540,"elapsed":16,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}},"outputId":"aefad1a8-6e0c-4e34-b988-95e8b2119b21"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of GPUs:  1\n","Does GPU exist? :  True\n"]}],"source":["# 사용할 GPU 지정\n","print(\"number of GPUs: \", torch.cuda.device_count())\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","use_cuda = torch.cuda.is_available()\n","print(\"Does GPU exist? : \", use_cuda)\n","DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"]},{"cell_type":"code","execution_count":8,"id":"3ffbcad1-d170-462f-807d-41760e65f4a0","metadata":{"id":"3ffbcad1-d170-462f-807d-41760e65f4a0","executionInfo":{"status":"ok","timestamp":1645583542458,"user_tz":-540,"elapsed":326,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}}},"outputs":[],"source":["# True 일 때 코드를 실행하면 example 등을 보여줌\n","DEBUG = False"]},{"cell_type":"code","execution_count":9,"id":"a7484847-924d-4f99-8b41-190ae4f192e8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7484847-924d-4f99-8b41-190ae4f192e8","executionInfo":{"status":"ok","timestamp":1645583551540,"user_tz":-540,"elapsed":680,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}},"outputId":"2522fbae-131e-4b99-9f0d-659f36608f4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["config file loaded.\n","beomi/kcbert-base\n"]}],"source":["# config 파일 불러오기\n","config_path = os.path.join('/content/drive/MyDrive/AIConnect/NLP_classificaiton/baseline/config.json')\n","\n","def set_config(config_path):\n","    if os.path.lexists(config_path):\n","        with open(config_path) as f:\n","            args = AttrDict(json.load(f))\n","            print(\"config file loaded.\")\n","            print(args.pretrained_model)\n","    else:\n","        assert False, 'config json file cannot be found.. please check the path again.'\n","    \n","    return args\n","    \n","\n","# 코드 중간중간에 끼워넣어 리셋 가능\n","args = set_config(config_path)\n","\n","# 결과 저장 폴더 미리 생성\n","os.makedirs(args.result_dir, exist_ok=True)\n","os.makedirs(args.config_dir, exist_ok=True)"]},{"cell_type":"markdown","id":"7a04c99c-208b-46a1-82cd-a7c9c776c8ad","metadata":{"id":"7a04c99c-208b-46a1-82cd-a7c9c776c8ad"},"source":["# 2. EDA 및 데이터 전처리"]},{"cell_type":"code","execution_count":10,"id":"87df6631-40e2-4c33-bacc-6548f9459d05","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87df6631-40e2-4c33-bacc-6548f9459d05","executionInfo":{"status":"ok","timestamp":1645583730282,"user_tz":-540,"elapsed":313,"user":{"displayName":"김태정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13554148649959058019"}},"outputId":"8c0caa93-0524-43d6-9bfc-2e5d000c9440"},"outputs":[{"output_type":"stream","name":"stdout","text":["train 데이터 경로가 올바른가요? :  False\n"]}],"source":["# data 경로 설정  \n","train_path = os.path.join(args.data_dir,'train.csv')\n","\n","print(\"train 데이터 경로가 올바른가요? : \", os.path.lexists(train_path))\n"]},{"cell_type":"markdown","id":"c3123598-d707-467a-a380-99644a9a3637","metadata":{"id":"c3123598-d707-467a-a380-99644a9a3637"},"source":["### 2-1. Train 데이터 확인"]},{"cell_type":"code","execution_count":null,"id":"d3d5cb19-0e61-4358-9a9b-c690cbe8c6cd","metadata":{"id":"d3d5cb19-0e61-4358-9a9b-c690cbe8c6cd"},"outputs":[],"source":["train_df = pd.read_csv(train_path, encoding = 'UTF-8-SIG')\n","\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"id":"f10dee23-db6d-4fce-8b06-054c3457adaf","metadata":{"id":"f10dee23-db6d-4fce-8b06-054c3457adaf"},"outputs":[],"source":["len(train_df)"]},{"cell_type":"code","execution_count":null,"id":"5ecb32ed-e7f9-4640-89c2-87005a9aed31","metadata":{"id":"5ecb32ed-e7f9-4640-89c2-87005a9aed31"},"outputs":[],"source":["print(\"bias classes: \", train_df.bias.unique())\n","print(\"hate classes: \", train_df.hate.unique())"]},{"cell_type":"code","execution_count":null,"id":"faecbd1a-82ee-40ed-b81a-2984674b669f","metadata":{"id":"faecbd1a-82ee-40ed-b81a-2984674b669f"},"outputs":[],"source":["pd.crosstab(train_df.bias, train_df.hate, margins=True)"]},{"cell_type":"markdown","id":"40d3802b-a501-4adc-9f9a-6968df7684d3","metadata":{"id":"40d3802b-a501-4adc-9f9a-6968df7684d3"},"source":["### 2-2. Test 데이터 확인"]},{"cell_type":"code","execution_count":null,"id":"86211318-bfaa-47c7-9653-63a13905dc51","metadata":{"id":"86211318-bfaa-47c7-9653-63a13905dc51"},"outputs":[],"source":["test_path = os.path.join(args.data_dir,'test.csv')\n","print(\"test 데이터 경로가 올바른가요? : \", os.path.lexists(test_path))"]},{"cell_type":"code","execution_count":null,"id":"a713f829-09c9-468a-8266-4bd555e29906","metadata":{"id":"a713f829-09c9-468a-8266-4bd555e29906"},"outputs":[],"source":["test_df = pd.read_csv(test_path)\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"id":"6ad5fbb7-acb5-47cb-b0f3-b220bf78ab67","metadata":{"id":"6ad5fbb7-acb5-47cb-b0f3-b220bf78ab67"},"outputs":[],"source":["len(test_df)"]},{"cell_type":"markdown","id":"44bb5aa8-6284-4066-a69d-a1539c5287a5","metadata":{"id":"44bb5aa8-6284-4066-a69d-a1539c5287a5"},"source":["### 2-3. 데이터 전처리 (Label Encoding)\n","bias, hate 라벨들의 class를 정수로 변경하여 라벨 인코딩을 하기 위한 딕셔너리입니다."]},{"cell_type":"markdown","id":"11018420-84da-40ac-b892-4e47f20dd83f","metadata":{"id":"11018420-84da-40ac-b892-4e47f20dd83f"},"source":["- bias, hate 컬럼을 합쳐서 하나의 라벨로 만들기 "]},{"cell_type":"code","execution_count":null,"id":"38c61bc4-da4a-4fc3-9cff-62079051ed82","metadata":{"id":"38c61bc4-da4a-4fc3-9cff-62079051ed82"},"outputs":[],"source":["# 두 라벨의 가능한 모든 조합 만들기\n","combinations = np.array(np.meshgrid(train_df.bias.unique(), train_df.hate.unique())).T.reshape(-1,2)\n","\n","if DEBUG==True:\n","    print(combinations)"]},{"cell_type":"code","execution_count":null,"id":"3c1f00cb-8009-460f-8903-ca876d647ff5","metadata":{"id":"3c1f00cb-8009-460f-8903-ca876d647ff5"},"outputs":[],"source":["# bias, hate 컬럼을 합친 것\n","bias_hate = list(np.array([train_df['bias'].values, train_df['hate'].values]).T.reshape(-1,2))\n","\n","if DEBUG==True:\n","    print(bias_hate[:5])\n"]},{"cell_type":"code","execution_count":null,"id":"26376de0-2a54-44c2-a8af-f20025b9798b","metadata":{"id":"26376de0-2a54-44c2-a8af-f20025b9798b"},"outputs":[],"source":["labels = []\n","for i, arr in enumerate(bias_hate):\n","    for idx, elem in enumerate(combinations):\n","        if np.array_equal(elem, arr):\n","            labels.append(idx)\n","\n","train_df['label'] = labels\n","train_df.head()"]},{"cell_type":"markdown","id":"add80824-55c5-4ca3-9858-5aeee3ede98d","metadata":{"id":"add80824-55c5-4ca3-9858-5aeee3ede98d"},"source":["## 3. Dataset 로드"]},{"cell_type":"markdown","id":"e9956c57-af6a-4330-a531-a338be9da9c4","metadata":{"tags":[],"id":"e9956c57-af6a-4330-a531-a338be9da9c4"},"source":["### 3-0. Pre-trained tokenizer 탐색"]},{"cell_type":"code","execution_count":null,"id":"bd6f75f4-cdf7-4998-8b78-3aa716b7045b","metadata":{"id":"bd6f75f4-cdf7-4998-8b78-3aa716b7045b"},"outputs":[],"source":["# config.json 에서 지정 이름별로 가져올 라이브러리 지정\n","\n","TOKENIZER_CLASSES = {\n","    \"BertTokenizer\": BertTokenizer\n","}\n"]},{"cell_type":"markdown","id":"9a285fec-0b39-4dce-8dc0-6faa6b027c34","metadata":{"tags":[],"id":"9a285fec-0b39-4dce-8dc0-6faa6b027c34"},"source":["- Tokenizer 사용 예시"]},{"cell_type":"code","execution_count":null,"id":"7d0f1e57-caf8-4d67-a2a9-61284a793f77","metadata":{"id":"7d0f1e57-caf8-4d67-a2a9-61284a793f77"},"outputs":[],"source":["TOKENIZER = TOKENIZER_CLASSES[args.tokenizer_class].from_pretrained(args.pretrained_model)\n","if DEBUG==True:\n","    print(TOKENIZER)"]},{"cell_type":"code","execution_count":null,"id":"1e8cdb8e-41a8-4d51-8aa0-f305782416ef","metadata":{"id":"1e8cdb8e-41a8-4d51-8aa0-f305782416ef"},"outputs":[],"source":["if DEBUG == True:\n","    example = train_df['title'][0]\n","    print(TOKENIZER(example))"]},{"cell_type":"code","execution_count":null,"id":"f174365c-90da-4c32-b6b4-5d66525a0687","metadata":{"tags":[],"id":"f174365c-90da-4c32-b6b4-5d66525a0687"},"outputs":[],"source":["if DEBUG==True:\n","    print(TOKENIZER.encode(example),\"\\n\")\n","    \n","    # 토큰으로 나누기\n","    print(TOKENIZER.tokenize(example),\"\\n\")\n","    \n","    # 토큰 id로 매핑하기\n","    print(TOKENIZER.convert_tokens_to_ids(TOKENIZER.tokenize(example)))\n"]},{"cell_type":"markdown","id":"cbf1708e-aedb-402f-a9c3-20ebdbe537eb","metadata":{"id":"cbf1708e-aedb-402f-a9c3-20ebdbe537eb"},"source":["### 3-1. Dataset 만드는 함수 정의"]},{"cell_type":"code","execution_count":null,"id":"85e6f065-671c-435c-8e73-8cad3c1496a3","metadata":{"id":"85e6f065-671c-435c-8e73-8cad3c1496a3"},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df, tokenizer, max_len, mode = 'train'):\n","\n","        self.data = df\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.mode = mode\n","        \n","        if self.mode!='test':\n","            try: \n","                self.labels = df['label'].tolist()\n","            except:\n","                assert False, 'CustomDataset Error : \\'label\\' column does not exist in the dataframe'\n","     \n","    def __len__(self):\n","        return len(self.data)\n","                \n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        전체 데이터에서 특정 인덱스 (idx)에 해당하는 기사제목과 댓글 내용을 \n","        토크나이즈한 data('input_ids', 'attention_mask','token_type_ids')의 딕셔너리 형태로 불러옴\n","        \"\"\"\n","        title = self.data.title.iloc[idx]\n","        comment = self.data.comment.iloc[idx]\n","        \n","        tokenized_text = self.tokenizer(title, comment,\n","                             padding= 'max_length',\n","                             max_length=self.max_len,\n","                             truncation=True,\n","                             return_token_type_ids=True,\n","                             return_attention_mask=True,\n","                             return_tensors = \"pt\")\n","        \n","        data = {'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n","               'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n","               'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long(),\n","               }\n","        \n","        if self.mode != 'test':\n","            label = self.data.label.iloc[idx]\n","            return data, label\n","        else:\n","            return data\n","        \n","\n","    \n","train_dataset = CustomDataset(train_df, TOKENIZER, args.max_seq_len, mode ='train')\n","print(\"train dataset loaded.\")"]},{"cell_type":"code","execution_count":null,"id":"7d27241c-4c3b-498e-9456-871dc3cc2e61","metadata":{"id":"7d27241c-4c3b-498e-9456-871dc3cc2e61"},"outputs":[],"source":["if DEBUG ==True :\n","    print(\"dataset sample : \")\n","    print(train_dataset[0])"]},{"cell_type":"code","execution_count":null,"id":"bbadd376-0df2-455c-89f1-9236ba9d32a0","metadata":{"id":"bbadd376-0df2-455c-89f1-9236ba9d32a0"},"outputs":[],"source":["# encoded_plus = tokenizer.encode_plus(\n","#                     sentence,                      # Sentence to encode.\n","#                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","#                     max_length = 128,           # Pad & truncate all sentences.\n","#                     pad_to_max_length = True,\n","#                     return_attention_mask = True,   # Construct attention masks.\n","#                     return_tensors = 'pt',     # Return pytorch tensors.\n","#                )"]},{"cell_type":"markdown","id":"e92d12dd-7a1e-423f-8b3d-00bae3c17375","metadata":{"id":"e92d12dd-7a1e-423f-8b3d-00bae3c17375"},"source":["### 3-2. Train, Validation set 나누기"]},{"cell_type":"code","execution_count":null,"id":"e6fb9525-861d-48ba-b495-828b8a08b9c1","metadata":{"id":"e6fb9525-861d-48ba-b495-828b8a08b9c1"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","                                                         \n","train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n","\n","train_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'train')\n","val_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'validation')\n","\n","print(\"Train dataset: \", len(train_dataset))\n","print(\"Validation dataset: \", len(val_dataset))"]},{"cell_type":"markdown","id":"ed493727-edd8-43ba-924f-195837306952","metadata":{"id":"ed493727-edd8-43ba-924f-195837306952"},"source":["## 4. 분류 모델 학습을 위한 세팅"]},{"cell_type":"markdown","id":"81c0817a-8b51-4d22-9f31-eb8bd22640cb","metadata":{"id":"81c0817a-8b51-4d22-9f31-eb8bd22640cb"},"source":["### 4-1. BertForSequenceClassification 설정"]},{"cell_type":"markdown","id":"5a250764-25aa-43e4-a383-f464dc54bc25","metadata":{"id":"5a250764-25aa-43e4-a383-f464dc54bc25"},"source":["\n","(https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)\n","\n","[PretrainedConfig](https://huggingface.co/transformers/v3.0.2/main_classes/configuration.html)\n"]},{"cell_type":"code","execution_count":null,"id":"554c0acd-d158-4d36-bfd1-abb47fb618f8","metadata":{"tags":[],"id":"554c0acd-d158-4d36-bfd1-abb47fb618f8"},"outputs":[],"source":["from transformers import logging\n","logging.set_verbosity_error()\n","\n","# config.json 에 입력된 architecture 에 따라 베이스 모델 설정\n","BASE_MODELS = {\n","    \"BertForSequenceClassification\": BertForSequenceClassification\n","}\n","\n","\n","myModel = BASE_MODELS[args.architecture].from_pretrained(args.pretrained_model, \n","                                                         num_labels = args.num_classes, \n","                                                         output_attentions = False, # Whether the model returns attentions weights.\n","                                                         output_hidden_states = True # Whether the model returns all hidden-states.\n","                                                        )\n","if DEBUG==True:\n","    # 모델 구조 확인\n","    print(myModel)"]},{"cell_type":"code","execution_count":null,"id":"c5f90f3c-51f6-4b44-a426-a0b36ccfa852","metadata":{"tags":[],"id":"c5f90f3c-51f6-4b44-a426-a0b36ccfa852"},"outputs":[],"source":["# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"]},{"cell_type":"markdown","id":"c022a093-130f-40ac-b8e5-8decc5c63a6d","metadata":{"id":"c022a093-130f-40ac-b8e5-8decc5c63a6d"},"source":["### 4-2. 모델 설정"]},{"cell_type":"markdown","id":"69d37b1b-d491-4d3c-8551-980f78a78337","metadata":{"id":"69d37b1b-d491-4d3c-8551-980f78a78337"},"source":["\n","BertForSequenceClassifier (line 1232부터 참고) [source code](https://github.com/huggingface/transformers/blob/a39dfe4fb122c11be98a563fb8ca43b322e01036/src/transformers/modeling_bert.py#L1284-L1287)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"1685ce12-7238-4e2a-a20d-11969b568807","metadata":{"id":"1685ce12-7238-4e2a-a20d-11969b568807"},"outputs":[],"source":["class myClassifier(nn.Module):\n","    def __init__(self, model, hidden_size = 768, num_classes=args.num_classes, dr_rate=None, params=None):\n","        super(myClassifier, self).__init__()\n","        self.model = model\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, token_ids, attention_mask, segment_ids):      \n","        outputs = self.model(input_ids = token_ids, \n","                             token_type_ids = segment_ids.long(), \n","                             attention_mask = attention_mask.float().to(token_ids.device))\n","         \n","        logits = outputs.logits\n","        output = self.softmax(logits)\n","        return output\n","        \n","model = myClassifier(myModel, dr_rate=0.1)\n","\n","# if DEBUG ==True :\n","#     print(model)"]},{"cell_type":"markdown","id":"99386a8c-7795-454c-9a32-eaf447c6fc4c","metadata":{"id":"99386a8c-7795-454c-9a32-eaf447c6fc4c"},"source":["### 4-3. 모델 구성 확인"]},{"cell_type":"code","execution_count":null,"id":"e3b6bc6f-1df3-46fb-a607-5de3e3cef571","metadata":{"id":"e3b6bc6f-1df3-46fb-a607-5de3e3cef571"},"outputs":[],"source":["params = list(model.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","id":"96c441a1-2ce3-46b2-b52d-c0cce7f76873","metadata":{"id":"96c441a1-2ce3-46b2-b52d-c0cce7f76873"},"source":["## 5. 학습 진행"]},{"cell_type":"markdown","id":"d4643714-c372-463a-8f7e-2955b2d4376a","metadata":{"id":"d4643714-c372-463a-8f7e-2955b2d4376a"},"source":["### 5-0. Early Stopper 함수 정의"]},{"cell_type":"code","execution_count":null,"id":"c205350d-ce58-42e7-9558-2b405f142ff4","metadata":{"id":"c205350d-ce58-42e7-9558-2b405f142ff4"},"outputs":[],"source":["class LossEarlyStopper():\n","    \"\"\"Early stopper\n","\n","        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n","        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n","        min_loss (float): 최소 loss\n","        stop (bool): True 일 때 학습 중단\n","\n","    \"\"\"\n","\n","    def __init__(self, patience: int)-> None:\n","        \"\"\" 초기화\n","\n","        Args:\n","            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n","            weight_path (str): weight 저장경로\n","            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n","        \"\"\"\n","        self.patience = patience\n","        self.patience_counter = 0\n","        self.min_loss = np.Inf\n","        self.stop = False\n","\n","    def check_early_stopping(self, loss: float)-> None:\n","        msg = ''\n","        # 첫 에폭\n","        if self.min_loss == np.Inf:\n","            self.min_loss = loss\n","           \n","        # loss가 줄지 않는다면 -> patience_counter 1 증가\n","        elif loss > self.min_loss:\n","            self.patience_counter += 1\n","            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n","\n","            # patience 만큼 loss가 줄지 않았다면 학습을 중단합니다.\n","            if self.patience_counter == self.patience:\n","                self.stop = True\n","  \n","        # loss가 줄어듬 -> min_loss 갱신, patience_counter 초기화\n","        elif loss <= self.min_loss:\n","            self.patience_counter = 0\n","            self.save_model = True\n","            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n","            self.min_loss = loss\n","\n","        print(msg)"]},{"cell_type":"markdown","id":"27d11163-6def-448a-9095-19231e9efe64","metadata":{"id":"27d11163-6def-448a-9095-19231e9efe64"},"source":["### 5-1. Epoch 별 학습 및 검증"]},{"cell_type":"markdown","id":"c06ed2a1-c2e3-43cb-9967-05d85b8546ee","metadata":{"id":"c06ed2a1-c2e3-43cb-9967-05d85b8546ee"},"source":["- Adam optimizer의 epsilon 파라미터 eps = 1e-8 는 \"계산 중 0으로 나눔을 방지 하기 위한 아주 작은 숫자 \" 입니다. ([출처](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n","- `warmup_ratio` : \n","  - 학습이 진행되면서 학습률을 그 상황에 맞게 가변적으로 적당하게 변경되게 하기 위해 Scheduler를 사용합니다.\n","  - 처음 학습률(Learning rate)를 warm up하기 위한 비율을 설정하는 warmup_ratio을 설정합니다."]},{"cell_type":"code","execution_count":null,"id":"d151484e-978d-4bf3-b7b3-fe2c5ba5eda5","metadata":{"tags":[],"id":"d151484e-978d-4bf3-b7b3-fe2c5ba5eda5"},"outputs":[],"source":["# args = set_config(config_path)\n","\n","logging.set_verbosity_warning()\n","\n","# 재현을 위해 모든 곳의 시드 고정\n","seed_val = args.seed\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","def train(model, train_data, val_data, args, mode = 'train'):\n","    \n","    # args.run은 실험 이름 (어디까지나 팀원들간의 버전 관리 및 공유 편의를 위한 것으로, 자유롭게 수정 가능합니다.)\n","    print(\"RUN : \", args.run)\n","    shutil.copyfile(\"config.json\", os.path.join(args.config_dir, f\"config_{args.run}.json\"))\n","\n","    early_stopper = LossEarlyStopper(patience=args.patience)\n","    \n","    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n","    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.train_batch_size)\n","\n","    DEBUG=False\n","    \n","    if DEBUG == True:\n","        # 데이터로더가 성공적으로 로드 되었는지 확인\n","        for idx, data in enumerate(train_dataloader):\n","            if idx==0:\n","                print(\"batch size : \", len(data[0]['input_ids']))\n","                print(\"The first batch looks like ..\\n\", data[0])\n","    \n","    \n","    criterion = nn.CrossEntropyLoss()\n","    \n","    total_steps = len(train_dataloader) * args.train_epochs\n","\n","    optimizer = Adam(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * args.warmup_proportion), num_training_steps=total_steps)\n","\n","    \n","    if use_cuda:\n","        model = model.to(DEVICE)\n","        criterion = criterion.to(DEVICE)\n","        \n","    model.train()\n","    optimizer.zero_grad()\n","\n","    tr_loss = 0.0\n","    val_loss = 0\n","    best_score = 0\n","      \n","\n","    for epoch_num in range(args.train_epochs):\n","\n","            total_acc_train = 0\n","            total_loss_train = 0\n","            \n","            assert mode in ['train', 'val'], 'your mode should be either \\'train\\' or \\'val\\''\n","            \n","            if mode =='train':\n","                for train_input, train_label in tqdm(train_dataloader):\n","                    \n","                    mask = train_input['attention_mask'].to(DEVICE)\n","                    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n","                    segment_ids = train_input['token_type_ids'].squeeze(1).to(DEVICE)\n","                    train_label = train_label.long().to(DEVICE)                \n","\n","                    output = model(input_id, mask, segment_ids)\n","                    \n","                    batch_loss = criterion(output.view(-1,6), train_label.view(-1))\n","                    total_loss_train += batch_loss.item()\n","\n","                    acc = (output.argmax(dim=1) == train_label).sum().item()\n","                    total_acc_train += acc\n","\n","                    model.zero_grad()\n","                    batch_loss.backward()\n","                    optimizer.step()\n","                    \n","\n","            total_acc_val = 0\n","            total_loss_val = 0\n","            \n","            # validation을 위해 이걸 넣으면 이 evaluation 프로세스 중엔 dropout 레이어가 다르가 동작한다.\n","            model.eval()\n","            \n","            with torch.no_grad():\n","\n","                for val_input, val_label in val_dataloader:\n","\n","                    mask = val_input['attention_mask'].to(DEVICE)\n","                    input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n","                    segment_ids = val_input['token_type_ids'].squeeze(1).to(DEVICE)\n","                    val_label = val_label.long().to(DEVICE)\n","\n","                    output = model(input_id, mask, segment_ids)\n","\n","                    batch_loss = criterion(output.view(-1,6), val_label.view(-1))\n","                    total_loss_val += batch_loss.item()\n","                    \n","                    acc = (output.argmax(dim=1) == val_label).sum().item()\n","                    total_acc_val += acc\n","            \n","            \n","            train_loss = total_loss_train / len(train_data)\n","            train_accuracy = total_acc_train / len(train_data)\n","            val_loss = total_loss_val / len(val_data)\n","            val_accuracy = total_acc_val / len(val_data)\n","            \n","            # 한 Epoch 학습 후 학습/검증에 대해 loss와 평가지표 (여기서는 accuracy로 임의로 설정) 출력\n","            print(\n","                f'Epoch: {epoch_num + 1} \\\n","                | Train Loss: {train_loss: .3f} \\\n","                | Train Accuracy: {train_accuracy: .3f} \\\n","                | Val Loss: {val_loss: .3f} \\\n","                | Val Accuracy: {val_accuracy: .3f}')\n","          \n","            # early_stopping check\n","            early_stopper.check_early_stopping(loss=val_loss)\n","\n","            if early_stopper.stop:\n","                print('Early stopped, Best score : ', best_score)\n","                break\n","\n","            if val_accuracy > best_score:\n","            # 모델이 개선됨 -> 검증 점수와 weight 갱신\n","                best_score = val_accuracy\n","                \n","                # 학습된 모델을 저장할 디렉토리 및 모델 이름 지정\n","                SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n","            \n","                check_point = {\n","                    'model': model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'scheduler': scheduler.state_dict()\n","                }\n","                torch.save(check_point, SAVED_MODEL)  \n","              \n","\n","\n","            \n","\n","\n","train(model, train_dataset, val_dataset, args, mode = 'train')"]},{"cell_type":"markdown","id":"6301514c-7d9e-4f37-9e8c-db85e3819b8b","metadata":{"id":"6301514c-7d9e-4f37-9e8c-db85e3819b8b"},"source":["## 6. Test dataset으로 추론 (Prediction)"]},{"cell_type":"code","execution_count":null,"id":"32696c34-b5e6-43b3-a1e7-ed8c2847baa9","metadata":{"id":"32696c34-b5e6-43b3-a1e7-ed8c2847baa9"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# 테스트 데이터셋 불러오기\n","test_data = CustomDataset(test_df, tokenizer = TOKENIZER, max_len= args.max_seq_len, mode='test')\n","\n","def test(model, SAVED_MODEL, test_data, args, mode = 'test'):\n","\n","\n","    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.eval_batch_size)\n","\n","\n","    if use_cuda:\n","\n","        model = model.to(DEVICE)\n","        model.load_state_dict(torch.load(SAVED_MODEL)['model'])\n","\n","\n","    model.eval()\n","\n","    pred = []\n","\n","    with torch.no_grad():\n","        for test_input in test_dataloader:\n","\n","            mask = test_input['attention_mask'].to(DEVICE)\n","            input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n","            segment_ids = test_input['token_type_ids'].squeeze(1).to(DEVICE)\n","\n","            output = model(input_id, mask, segment_ids)\n","\n","            output = output.argmax(dim=1).cpu().tolist()\n","\n","            for label in output:\n","                pred.append(label)\n","                \n","    return pred\n","\n","SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n","\n","pred = test(model, SAVED_MODEL, test_data, args)"]},{"cell_type":"code","execution_count":null,"id":"29432647-7a5d-4dda-99af-ad6f151fd2eb","metadata":{"id":"29432647-7a5d-4dda-99af-ad6f151fd2eb"},"outputs":[],"source":["print(\"prediction completed for \", len(pred), \"comments\")\n"]},{"cell_type":"markdown","id":"fe8aaf8d-93c6-4998-b005-f32ec5986f46","metadata":{"id":"fe8aaf8d-93c6-4998-b005-f32ec5986f46"},"source":["### "]},{"cell_type":"code","execution_count":null,"id":"5ccc3c34-e3a8-4172-8b16-f1c2a5bb5a3e","metadata":{"id":"5ccc3c34-e3a8-4172-8b16-f1c2a5bb5a3e"},"outputs":[],"source":["# 0-5 사이의 라벨 값 별로 bias, hate로 디코딩 하기 위한 딕셔너리\n","bias_dict = {0: 'none', 1: 'none', 2: 'others', 3:'others', 4:'gender', 5:'gender'}\n","hate_dict = {0: 'none', 1: 'hate', 2: 'none', 3:'hate', 4:'none', 5:'hate'}\n","\n","# 인코딩 값으로 나온 타겟 변수를 디코딩\n","pred_bias = ['' for i in range(len(pred))]\n","pred_hate = ['' for i in range(len(pred))]\n","\n","for idx, label in enumerate(pred):\n","    pred_bias[idx]=(str(bias_dict[label]))\n","    pred_hate[idx]=(str(hate_dict[label]))\n","print('decode Completed!')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"75e6cdb3-8245-468e-aaf0-4e6f328e98c9","metadata":{"id":"75e6cdb3-8245-468e-aaf0-4e6f328e98c9"},"outputs":[],"source":["submit = pd.read_csv(os.path.join(args.data_dir,'sample_submission.csv'))\n","\n","submit['bias'] = pred_bias\n","submit['hate'] = pred_hate\n","submit"]},{"cell_type":"code","execution_count":null,"id":"964b5159-8ffe-4cc2-833b-3c4b0c857022","metadata":{"id":"964b5159-8ffe-4cc2-833b-3c4b0c857022"},"outputs":[],"source":["submit.to_csv(os.path.join(args.result_dir, f\"submission_{args.run}.csv\"), index=False)"]},{"cell_type":"code","execution_count":null,"id":"e4a30767-7803-448a-950b-412b71088d07","metadata":{"id":"e4a30767-7803-448a-950b-412b71088d07"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"f89023af-867b-4d28-b30d-e465d0abe66a","metadata":{"id":"f89023af-867b-4d28-b30d-e465d0abe66a"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"01_nlp_baseline.ipynb","provenance":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}