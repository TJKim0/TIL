# Transformer
* attention 을 활용한 딥러닝 모델

* seq2seq 과제 수행에 최적화된 모델.

* 트랜스포머의 최종 출력, 즉 디코더 출력은 타깃 언어의 어휘 수만큼의 차원으로 구성된 벡터(vector)입니다. 이 벡터의 특징은 요솟(element)값이 모두 '확률'이라는 점입니다. 예를 들어 타깃 언어의 어휘가 총 3만개라고 가정해 보면 디코더 출력은 3만 차원의 벡터입니다. 이 벡터의 요솟값 3만 개는 각각은 확률이므로 0 이상 1 이하의 값을 가지며 모두 더하면 1이 됩니다.

트랜스포머의 학습(train)은 인코더와 디코더 입력이 주어졌을 때 정답에 해당하는 단어의 확률 값을 높이는 방식으로 수행됩니다. 이를 나타낸 다음 그림을 보면 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록, 모델 전체를 갱신합니다.