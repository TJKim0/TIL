{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "* 주변의 데이터를 보고 비교할 때 유리한 방식.\n",
    "* 주로 classification 에서 많이 쓰인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer normalization\n",
    "* 주변 데이터와 상관이 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizaiton\n",
    "* 모델 파라미터가 너무 큰 값을 가지지 않게 해주는 역할.\n",
    "## L1 regularization(Lasso)\n",
    "* 파라미터 값이 크더라도 그 크기에 상관없이 똑같이 영향\n",
    "* w1: 10, w2: 100 -> 똑같이 영향을 받는다.\n",
    "* w1: 0.00001, w2: 100 -> w1 이 0 이 되어버린다.\n",
    "## L2 regularization(Ridge)\n",
    "* w1: 10 , w2 : 100 -> w2 가 10배 더 많이 줄어든다.\n",
    "* loss fucntion에 더해서 추가적으로 사용함.(보조 도구)\n",
    "* w1: 0.00001, w2: 100 -> w1 이 줄어드는 속도가 훨씬 작다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss : 손실 함수 (맞추고 싶은 값을 예측하도록 하는 도구)\n",
    "* L1 loss\n",
    "* L2 loss\n",
    "* 회귀 모델의 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류 모델의 loss는 CrossEntropyloss + L1/L2 regularization 이런식으로 섞어 쓰기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 대회 때 사용하는 스킬\n",
    "* kaggle 대회, dacon 대회\n",
    "1. State-of-the-art-model (가장 좋은 모델)\n",
    "2. Ansemble 사용\n",
    "    * 회귀 모델\n",
    "        * Median 값 사용\n",
    "            * outliar에 강함.\n",
    "        * Mean 값\n",
    "            * outliar 도 포함해서 평균을 내기 때문에 성능이 구데기.\n",
    "\n",
    "    * 분류 모델\n",
    "        * logit 합.\n",
    "        * Softmax probability 결과값을 평균.\n",
    "        * 분류 결과값 중 가장 많이 나온 것.\n",
    "    \n",
    "    * 방법\n",
    "        * train dataset 5분할\n",
    "        * 80% 씩 학습한 5개 모델을 만듦\n",
    "3. Feature를 뽑는 방법\n",
    "    * 어떤 feature를 사용할까 ?\n",
    "        * feature 를 모두 학습시킨 모델.\n",
    "        * feature 를 하나씩 빼면서 모델을 학습시킴ㅇㅇ    \n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상관관계가 높은 feature\n",
    "* 인과관계 / 상관관계 구분을 잘 해야함.\n",
    "* 아이스크림 판매량 -> 워터파크 매출(예측됨. 하지만 인과관계 X)  -- 제거 하는게 맞다.\n",
    "* 온도 -> 워터파크 매출 (인과관계 있음)\n",
    "* 키 - 몸무게 -> 비만에 의한 합병증.(x)\n",
    "* BMI 지수 -> 비만에 의한 합병증.(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector 의 개념. 딥러닝에서 내적이 가지는 의미.\n",
    "* Gilbert strang  - linear algebra 강의 추천.\n",
    "* 내적(dot product)\n",
    "    * v1 . v2 = |v1||v2| cos (theta)\n",
    "    * 두직선의 관계성.\n",
    "    * Transformer key, query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
