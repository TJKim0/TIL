{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12232,"status":"ok","timestamp":1642600787027,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"fUI73fQklt2r","outputId":"94993be2-f647-4b6e-e26b-59bcd43e87fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch_transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.10.0+cu111)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.62.3)\n","Collecting boto3\n","  Downloading boto3-1.20.38-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 46.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.0.0-\u003epytorch_transformers) (3.10.0.2)\n","Collecting botocore\u003c1.24.0,\u003e=1.23.38\n","  Downloading botocore-1.23.38-py3-none-any.whl (8.5 MB)\n","\u001b[K     |████████████████████████████████| 8.5 MB 40.5 MB/s \n","\u001b[?25hCollecting s3transfer\u003c0.6.0,\u003e=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.7 MB/s \n","\u001b[?25hCollecting jmespath\u003c1.0.0,\u003e=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting urllib3\u003c1.27,\u003e=1.25.4\n","  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 53.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.24.0,\u003e=1.23.38-\u003eboto3-\u003epytorch_transformers) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003c3.0.0,\u003e=2.1-\u003ebotocore\u003c1.24.0,\u003e=1.23.38-\u003eboto3-\u003epytorch_transformers) (1.15.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_transformers) (2021.10.8)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 51.5 MB/s \n","\u001b[?25hRequirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_transformers) (2.10)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003epytorch_transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-\u003epytorch_transformers) (7.1.2)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.38 botocore-1.23.38 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.47 sentencepiece-0.1.96 urllib3-1.25.11\n"]}],"source":["!pip install pytorch_transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3449,"status":"ok","timestamp":1642600790474,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"ceCTG8TymGow","outputId":"3f11f549-c393-4fbc-a1a5-0034d5ba690b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.20.38)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: torch\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=0.4.1-\u003epytorch_pretrained_bert) (3.10.0.2)\n","Requirement already satisfied: jmespath\u003c1.0.0,\u003e=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch_pretrained_bert) (0.10.0)\n","Requirement already satisfied: s3transfer\u003c0.6.0,\u003e=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch_pretrained_bert) (0.5.0)\n","Requirement already satisfied: botocore\u003c1.24.0,\u003e=1.23.38 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch_pretrained_bert) (1.23.38)\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.24.0,\u003e=1.23.38-\u003eboto3-\u003epytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.24.0,\u003e=1.23.38-\u003eboto3-\u003epytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003c3.0.0,\u003e=2.1-\u003ebotocore\u003c1.24.0,\u003e=1.23.38-\u003eboto3-\u003epytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_pretrained_bert) (2021.10.8)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch_pretrained_bert) (2.10)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n"]}],"source":["!pip install pytorch_pretrained_bert"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5704,"status":"ok","timestamp":1642600796175,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"2PHYS6-Repct"},"outputs":[],"source":["import torch\n","from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM # Load BertTokenizer and the model \n","\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","logging.basicConfig(level=logging.INFO)\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642600796473,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"FVe-tOVJepcw"},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","from torch.optim import lr_scheduler # learning rate scheduler module \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","from random import randrange\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"RIO6NTfNepcx"},"source":["## Load Pre-trained model Tokenizer "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1642600796911,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"2i8kiSYDepcz","outputId":"b134eadb-65d6-4815-b084-b6560b37375d"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpldxs6chh\n","100%|██████████| 231508/231508 [00:00\u003c00:00, 2147768.94B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmpldxs6chh to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpldxs6chh\n","INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"]}],"source":["# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # set the tokenizer "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1642600796911,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"hnL2tUDlepc0"},"outputs":[],"source":["text ='what is a pug'"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1642600796912,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"MpeTIAbbepc0"},"outputs":[],"source":["tokenizer_result = tokenizer.tokenize(text)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1642600796912,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"eOSKDe-Sepc1","outputId":"fac2661a-8d69-4161-8049-783b1f3b9a45"},"outputs":[{"data":{"text/plain":["['what', 'is', 'a', 'pu', '##g']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_result"]},{"cell_type":"markdown","metadata":{"id":"8t7JZJitepc2"},"source":["## Build Model"]},{"cell_type":"markdown","metadata":{"id":"5IZ8_-BGepc3"},"source":["![대체 텍스트](./figures/bert_sentence.png)"]},{"cell_type":"markdown","metadata":{"id":"r0tYMPXtepc4"},"source":["![대체 텍스트](./figures/bert_inputs.png)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1642600796912,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"nsuHrFpbepc4"},"outputs":[],"source":["\n","class BertForSequenceClassification(nn.Module):\n","    \"\"\"BERT model for classification.\n","    This module is composed of the BERT model with a linear layer on top of\n","    the pooled output.\n","    Params:\n","        `config`: a BertConfig class instance with the configuration to build a new model.\n","        `num_labels`: the number of classes for the classifier. Default = 2.\n","    Inputs:\n","        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n","            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n","            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n","        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n","            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n","            a `sentence B` token (see BERT paper for more details).\n","        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n","            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n","            input sequence length in the current batch. It's the mask that we typically use for attention when\n","            a batch has varying length sentences.\n","        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n","            with indices selected in [0, ..., num_labels].\n","    Outputs:\n","        if `labels` is not `None`:\n","            Outputs the CrossEntropy classification loss of the output with the labels.\n","        if `labels` is `None`:\n","            Outputs the classification logits of shape [batch_size, num_labels].\n","    Example usage:\n","    ```python\n","    # Already been converted into WordPiece token ids\n","    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","    num_labels = 2\n","    model = BertForSequenceClassification(config, num_labels)\n","    logits = model(input_ids, token_type_ids, input_mask)\n","    ```\n","    \"\"\"\n","    def __init__(self, num_labels=2):\n","        super(BertForSequenceClassification, self).__init__()\n","        self.num_labels = num_labels\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        nn.init.xavier_normal_(self.classifier.weight)\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n","        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits\n","    def freeze_bert_encoder(self):\n","        for param in self.bert.parameters():\n","            param.requires_grad = False\n","    \n","    def unfreeze_bert_encoder(self):\n","        for param in self.bert.parameters():\n","            param.requires_grad = True"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16364,"status":"ok","timestamp":1642600813271,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"ZVczbNYPepc6","outputId":"c72a6a2a-a663-4fa6-bb68-1190d4a2d5d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:pytorch_pretrained_bert.modeling:Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n","INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpjh2iy0qi\n","100%|██████████| 433/433 [00:00\u003c00:00, 413415.35B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmpjh2iy0qi to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpjh2iy0qi\n","INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n","INFO:pytorch_transformers.modeling_utils:Model config {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 2,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pad_token_id\": 0,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpr16q1lkl\n","100%|██████████| 440473133/440473133 [00:10\u003c00:00, 40065925.84B/s]\n","INFO:pytorch_transformers.file_utils:copying /tmp/tmpr16q1lkl to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpr16q1lkl\n","INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"]}],"source":["from pytorch_pretrained_bert import BertConfig\n","\n","config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","num_labels = 2\n","model = BertForSequenceClassification(num_labels)\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokenizer_result)])\n","\n","logits = model(tokens_tensor)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1642600813271,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"X0oWy739epc6","outputId":"d031cebd-c877-4a4f-ba3d-6665a2863c36"},"outputs":[{"data":{"text/plain":["tensor([[-0.5009, -0.4781]], grad_fn=\u003cAddmmBackward0\u003e)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["logits"]},{"cell_type":"markdown","metadata":{"id":"Qy9bGJgWepc6"},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16023,"status":"ok","timestamp":1642600829273,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"nUc6Z6-ImmAN","outputId":"827f90ad-8aca-426d-904b-5e8119727e08"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2485,"status":"ok","timestamp":1642600832012,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"sJiwGMxRepc7"},"outputs":[],"source":["import os\n","import pandas as pd\n","data_path = '/content/drive/MyDrive/중진공 이어드림/1 20 nlp/실습2'\n","\n","dat = pd.read_csv(os.path.join(data_path, 'Data/IMDB Dataset.csv'))"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1642600832012,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"8W7u04DFepc7","outputId":"f7cc77fa-5583-4bf6-f40b-17ca9836456b"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-ed902971-302d-4ef7-a3bf-7e26743c0d22\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ereview\u003c/th\u003e\n","      \u003cth\u003esentiment\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eOne of the other reviewers has mentioned that ...\u003c/td\u003e\n","      \u003ctd\u003epositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eA wonderful little production. The filming tec...\u003c/td\u003e\n","      \u003ctd\u003epositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eI thought this was a wonderful way to spend ti...\u003c/td\u003e\n","      \u003ctd\u003epositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eBasically there's a family where a little boy ...\u003c/td\u003e\n","      \u003ctd\u003enegative\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003ePetter Mattei's \"Love in the Time of Money\" is...\u003c/td\u003e\n","      \u003ctd\u003epositive\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed902971-302d-4ef7-a3bf-7e26743c0d22')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-ed902971-302d-4ef7-a3bf-7e26743c0d22 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed902971-302d-4ef7-a3bf-7e26743c0d22');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. The filming tec...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["dat.head()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1642600832012,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"6Rroiq7Eepc7","outputId":"09ca69e6-8fef-4caf-81bf-6bbe0a108842","scrolled":true},"outputs":[{"data":{"text/plain":["[['a',\n","  'wonderful',\n","  'little',\n","  'production',\n","  '.',\n","  'the',\n","  'filming',\n","  'technique',\n","  'is',\n","  'very',\n","  'una',\n","  '##ss',\n","  '##uming',\n","  '-',\n","  'very',\n","  'old',\n","  '-',\n","  'time',\n","  '-',\n","  'bbc',\n","  'fashion',\n","  'and',\n","  'gives',\n","  'a',\n","  'comforting',\n","  ',',\n","  'and',\n","  'sometimes',\n","  'discomfort',\n","  '##ing',\n","  ',',\n","  'sense',\n","  'of',\n","  'realism',\n","  'to',\n","  'the',\n","  'entire',\n","  'piece',\n","  '.',\n","  'the',\n","  'actors',\n","  'are',\n","  'extremely',\n","  'well',\n","  'chosen',\n","  '-',\n","  'michael',\n","  'sheen',\n","  'not',\n","  'only',\n","  '\"',\n","  'has',\n","  'got',\n","  'all',\n","  'the',\n","  'polar',\n","  '##i',\n","  '\"',\n","  'but',\n","  'he',\n","  'has',\n","  'all',\n","  'the',\n","  'voices',\n","  'down',\n","  'pat',\n","  'too',\n","  '!',\n","  'you',\n","  'can',\n","  'truly',\n","  'see',\n","  'the',\n","  'seam',\n","  '##less',\n","  'editing',\n","  'guided',\n","  'by',\n","  'the',\n","  'references',\n","  'to',\n","  'williams',\n","  \"'\",\n","  'diary',\n","  'entries',\n","  ',',\n","  'not',\n","  'only',\n","  'is',\n","  'it',\n","  'well',\n","  'worth',\n","  'the',\n","  'watching',\n","  'but',\n","  'it',\n","  'is',\n","  'a',\n","  'terrific',\n","  '##ly',\n","  'written',\n","  'and',\n","  'performed',\n","  'piece',\n","  '.',\n","  'a',\n","  'master',\n","  '##ful',\n","  'production',\n","  'about',\n","  'one',\n","  'of',\n","  'the',\n","  'great',\n","  'master',\n","  \"'\",\n","  's',\n","  'of',\n","  'comedy',\n","  'and',\n","  'his',\n","  'life',\n","  '.',\n","  'the',\n","  'realism',\n","  'really',\n","  'comes',\n","  'home',\n","  'with',\n","  'the',\n","  'little',\n","  'things',\n","  ':',\n","  'the',\n","  'fantasy',\n","  'of',\n","  'the',\n","  'guard',\n","  'which',\n","  ',',\n","  'rather',\n","  'than',\n","  'use',\n","  'the',\n","  'traditional',\n","  \"'\",\n","  'dream',\n","  \"'\",\n","  'techniques',\n","  'remains',\n","  'solid',\n","  'then',\n","  'disappears',\n","  '.',\n","  'it',\n","  'plays',\n","  'on',\n","  'our',\n","  'knowledge',\n","  'and',\n","  'our',\n","  'senses',\n","  ',',\n","  'particularly',\n","  'with',\n","  'the',\n","  'scenes',\n","  'concerning',\n","  'orton',\n","  'and',\n","  'hall',\n","  '##i',\n","  '##well',\n","  'and',\n","  'the',\n","  'sets',\n","  '(',\n","  'particularly',\n","  'of',\n","  'their',\n","  'flat',\n","  'with',\n","  'hall',\n","  '##i',\n","  '##well',\n","  \"'\",\n","  's',\n","  'murals',\n","  'decor',\n","  '##ating',\n","  'every',\n","  'surface',\n","  ')',\n","  'are',\n","  'terribly',\n","  'well',\n","  'done',\n","  '.'],\n"," ['basically',\n","  'there',\n","  \"'\",\n","  's',\n","  'a',\n","  'family',\n","  'where',\n","  'a',\n","  'little',\n","  'boy',\n","  '(',\n","  'jake',\n","  ')',\n","  'thinks',\n","  'there',\n","  \"'\",\n","  's',\n","  'a',\n","  'zombie',\n","  'in',\n","  'his',\n","  'closet',\n","  '\u0026',\n","  'his',\n","  'parents',\n","  'are',\n","  'fighting',\n","  'all',\n","  'the',\n","  'time',\n","  '.',\n","  'this',\n","  'movie',\n","  'is',\n","  'slower',\n","  'than',\n","  'a',\n","  'soap',\n","  'opera',\n","  '.',\n","  '.',\n","  '.',\n","  'and',\n","  'suddenly',\n","  ',',\n","  'jake',\n","  'decides',\n","  'to',\n","  'become',\n","  'ram',\n","  '##bo',\n","  'and',\n","  'kill',\n","  'the',\n","  'zombie',\n","  '.',\n","  'ok',\n","  ',',\n","  'first',\n","  'of',\n","  'all',\n","  'when',\n","  'you',\n","  \"'\",\n","  're',\n","  'going',\n","  'to',\n","  'make',\n","  'a',\n","  'film',\n","  'you',\n","  'must',\n","  'decide',\n","  'if',\n","  'its',\n","  'a',\n","  'thriller',\n","  'or',\n","  'a',\n","  'drama',\n","  '!',\n","  'as',\n","  'a',\n","  'drama',\n","  'the',\n","  'movie',\n","  'is',\n","  'watch',\n","  '##able',\n","  '.',\n","  'parents',\n","  'are',\n","  'di',\n","  '##vor',\n","  '##cing',\n","  '\u0026',\n","  'arguing',\n","  'like',\n","  'in',\n","  'real',\n","  'life',\n","  '.',\n","  'and',\n","  'then',\n","  'we',\n","  'have',\n","  'jake',\n","  'with',\n","  'his',\n","  'closet',\n","  'which',\n","  'totally',\n","  'ruins',\n","  'all',\n","  'the',\n","  'film',\n","  '!',\n","  'i',\n","  'expected',\n","  'to',\n","  'see',\n","  'a',\n","  'boo',\n","  '##ge',\n","  '##yman',\n","  'similar',\n","  'movie',\n","  ',',\n","  'and',\n","  'instead',\n","  'i',\n","  'watched',\n","  'a',\n","  'drama',\n","  'with',\n","  'some',\n","  'meaningless',\n","  'thriller',\n","  'spots',\n","  '.',\n","  '3',\n","  'out',\n","  'of',\n","  '10',\n","  'just',\n","  'for',\n","  'the',\n","  'well',\n","  'playing',\n","  'parents',\n","  '\u0026',\n","  'descent',\n","  'dial',\n","  '##og',\n","  '##s',\n","  '.',\n","  'as',\n","  'for',\n","  'the',\n","  'shots',\n","  'with',\n","  'jake',\n","  ':',\n","  'just',\n","  'ignore',\n","  'them',\n","  '.']]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["review1 = tokenizer.tokenize(dat.review[1])\n","review3 = tokenizer.tokenize(dat.review[3])\n","\n","[review1,review3 ]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1642600832013,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"T3F4Xq_eepc8"},"outputs":[],"source":["tokens_sentence1 = tokenizer.convert_tokens_to_ids(review1)\n","tokens_sentence3 = tokenizer.convert_tokens_to_ids(review3)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1642600832013,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"A-W5s7Ppepc8","outputId":"b2dd0ac0-7061-4965-9dee-6d9c09e168fe"},"outputs":[{"data":{"text/plain":["[1037,\n"," 6919,\n"," 2210,\n"," 2537,\n"," 1012,\n"," 1996,\n"," 7467,\n"," 6028,\n"," 2003,\n"," 2200,\n"," 14477,\n"," 4757,\n"," 24270,\n"," 1011,\n"," 2200,\n"," 2214,\n"," 1011,\n"," 2051,\n"," 1011,\n"," 4035,\n"," 4827,\n"," 1998,\n"," 3957,\n"," 1037,\n"," 16334,\n"," 1010,\n"," 1998,\n"," 2823,\n"," 17964,\n"," 2075,\n"," 1010,\n"," 3168,\n"," 1997,\n"," 15650,\n"," 2000,\n"," 1996,\n"," 2972,\n"," 3538,\n"," 1012,\n"," 1996,\n"," 5889,\n"," 2024,\n"," 5186,\n"," 2092,\n"," 4217,\n"," 1011,\n"," 2745,\n"," 20682,\n"," 2025,\n"," 2069,\n"," 1000,\n"," 2038,\n"," 2288,\n"," 2035,\n"," 1996,\n"," 11508,\n"," 2072,\n"," 1000,\n"," 2021,\n"," 2002,\n"," 2038,\n"," 2035,\n"," 1996,\n"," 5755,\n"," 2091,\n"," 6986,\n"," 2205,\n"," 999,\n"," 2017,\n"," 2064,\n"," 5621,\n"," 2156,\n"," 1996,\n"," 25180,\n"," 3238,\n"," 9260,\n"," 8546,\n"," 2011,\n"," 1996,\n"," 7604,\n"," 2000,\n"," 3766,\n"," 1005,\n"," 9708,\n"," 10445,\n"," 1010,\n"," 2025,\n"," 2069,\n"," 2003,\n"," 2009,\n"," 2092,\n"," 4276,\n"," 1996,\n"," 3666,\n"," 2021,\n"," 2009,\n"," 2003,\n"," 1037,\n"," 27547,\n"," 2135,\n"," 2517,\n"," 1998,\n"," 2864,\n"," 3538,\n"," 1012,\n"," 1037,\n"," 3040,\n"," 3993,\n"," 2537,\n"," 2055,\n"," 2028,\n"," 1997,\n"," 1996,\n"," 2307,\n"," 3040,\n"," 1005,\n"," 1055,\n"," 1997,\n"," 4038,\n"," 1998,\n"," 2010,\n"," 2166,\n"," 1012,\n"," 1996,\n"," 15650,\n"," 2428,\n"," 3310,\n"," 2188,\n"," 2007,\n"," 1996,\n"," 2210,\n"," 2477,\n"," 1024,\n"," 1996,\n"," 5913,\n"," 1997,\n"," 1996,\n"," 3457,\n"," 2029,\n"," 1010,\n"," 2738,\n"," 2084,\n"," 2224,\n"," 1996,\n"," 3151,\n"," 1005,\n"," 3959,\n"," 1005,\n"," 5461,\n"," 3464,\n"," 5024,\n"," 2059,\n"," 17144,\n"," 1012,\n"," 2009,\n"," 3248,\n"," 2006,\n"," 2256,\n"," 3716,\n"," 1998,\n"," 2256,\n"," 9456,\n"," 1010,\n"," 3391,\n"," 2007,\n"," 1996,\n"," 5019,\n"," 7175,\n"," 25161,\n"," 1998,\n"," 2534,\n"," 2072,\n"," 4381,\n"," 1998,\n"," 1996,\n"," 4520,\n"," 1006,\n"," 3391,\n"," 1997,\n"," 2037,\n"," 4257,\n"," 2007,\n"," 2534,\n"," 2072,\n"," 4381,\n"," 1005,\n"," 1055,\n"," 19016,\n"," 25545,\n"," 5844,\n"," 2296,\n"," 3302,\n"," 1007,\n"," 2024,\n"," 16668,\n"," 2092,\n"," 2589,\n"," 1012]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tokens_sentence1"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1642600832013,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"2shRAXEGepc8","outputId":"441d8757-cc36-4229-dbf8-c0c222546174"},"outputs":[{"data":{"text/plain":["tensor([[0.4943, 0.5057]], grad_fn=\u003cSoftmaxBackward0\u003e)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn.functional as F\n","\n","F.softmax(logits,dim=1)"]},{"cell_type":"markdown","metadata":{"id":"metE7Lxtepc8"},"source":["## Defined Data and DataLoader (IMDB)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":671,"status":"ok","timestamp":1642600832677,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"QXH4SSgtepc8"},"outputs":[],"source":["# Splitting the data\n","from sklearn.model_selection import train_test_split\n","X = dat['review']\n","y = dat['sentiment']\n","\n","X_train = dat['review'][:30684] \n","y_train = dat['sentiment'][:30684]\n","X_test = dat['review'][30684:]\n","y_test = dat['sentiment'][30684:]\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1642600832677,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"r7qnNAVnepc9"},"outputs":[],"source":["X_train = X_train.values.tolist()\n","X_test = X_test.values.tolist()\n","\n","y_train = pd.get_dummies(y_train).values.tolist()\n","y_test = pd.get_dummies(y_test).values.tolist()\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1642600832678,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"H-w_HTuNepc9"},"outputs":[],"source":["# Make the Dataset object for the dataloader \n","max_seq_length = 256\n","class text_dataset(Dataset):\n","    def __init__(self,x_y_list, transform=None):\n","        \n","        self.x_y_list = x_y_list\n","        self.transform = transform\n","        \n","    def __getitem__(self,index):\n","        \n","        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index]) # tokenize the review \n","        \n","        if len(tokenized_review) \u003e max_seq_length: # check the length of the sentence \n","            tokenized_review = tokenized_review[:max_seq_length]\n","            \n","        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review) # conver to ids \n","\n","        padding = [0] * (max_seq_length - len(ids_review)) # add pad token \n","        \n","        ids_review += padding \n","        \n","        assert len(ids_review) == max_seq_length\n","        \n","        #print(ids_review)\n","        ids_review = torch.tensor(ids_review)\n","        \n","        sentiment = self.x_y_list[1][index] # color        \n","        list_of_labels = [torch.from_numpy(np.array(sentiment))]\n","        \n","        \n","        return ids_review, list_of_labels[0]\n","    \n","    def __len__(self):\n","        return len(self.x_y_list[0])"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1642600832678,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"Bdr0KP3kepc9","outputId":"43745a2b-43be-4d32-f96c-861b7528cc6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["\n","batch_size = 16\n","\n","train_lists = [X_train, y_train]\n","test_lists = [X_test, y_test]\n","\n","training_dataset = text_dataset(x_y_list = train_lists )\n","\n","test_dataset = text_dataset(x_y_list = test_lists )\n","\n","dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n","                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","                   }\n","dataset_sizes = {'train':len(train_lists[0]),\n","                'val':len(test_lists[0])}\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":396,"status":"ok","timestamp":1642600833071,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"673LOOixepc-"},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","    print('starting')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = 100\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                scheduler.step()\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            \n","            sentiment_corrects = 0\n","            \n","            \n","            # Iterate over data.\n","            for inputs, sentiment in dataloaders_dict[phase]:\n","                #inputs = inputs\n","                #print(len(inputs),type(inputs),inputs)\n","                #inputs = torch.from_numpy(np.array(inputs)).to(device) \n","                inputs = inputs.to(device) \n","\n","                sentiment = sentiment.to(device)\n","                \n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    #print(inputs)\n","                    outputs = model(inputs)\n","\n","                    outputs = F.softmax(outputs,dim=1)\n","                    \n","                    loss = criterion(outputs, torch.max(sentiment.float(), 1)[1])\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        \n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","\n","                \n","                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(sentiment, 1)[1])\n","\n","                \n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            \n","            sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n","            print('{} sentiment_acc: {:.4f}'.format(\n","                phase, sentiment_acc))\n","\n","            if phase == 'val' and epoch_loss \u003c best_loss:\n","                print('saving with loss of {}'.format(epoch_loss),\n","                      'improved over previous {}'.format(best_loss))\n","                best_loss = epoch_loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                torch.save(model.state_dict(), 'bert_model_test.pth')\n","\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val loss: {:4f}'.format(float(best_loss)))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9317,"status":"ok","timestamp":1642600842386,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"Vj_5zPp6epc-","outputId":"a7753c9f-9d62-4847-a55a-cb773b4c1cc2"},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model.to(device)\n","\n","#model.freeze_bert_encoder()\n","#model.classifier.weight.requires_grad = True\n","model\n","\n","#    def freeze_bert_encoder(self):\n","#        for param in self.bert.parameters():\n","#            param.requires_grad = False\n","#    \n","#    def unfreeze_bert_encoder(self):\n","#        for param in self.bert.parameters():\n","#            param.requires_grad = True"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1642600842387,"user":{"displayName":"j hy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_1aUAMVTi1NBHffYMoF0eDDM7EE1WyOtUCvE4Jg=s64","userId":"06504932993766290160"},"user_tz":-540},"id":"Twy_5sHqepc-"},"outputs":[],"source":["lrlast = .001\n","lrmain = .00001\n","optim1 = optim.Adam(\n","    [\n","        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n","        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n","       \n","   ])\n","\n","#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim1\n","criterion = nn.CrossEntropyLoss()\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dfhH0eSlepdA","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["starting\n","Epoch 0/9\n","----------\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]}],"source":["model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=10)\n"]},{"cell_type":"markdown","metadata":{"id":"WSOXfoueepdA"},"source":["### Only Fine-tuning the last linear layer "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7ackPoxepdA"},"outputs":[],"source":["lrlast = .001\n","lrmain = .00001\n","model.bert.eval() # eval mode (Bert)\n","optim1 = optim.Adam(\n","    [\n","        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n","       \n","   ])\n","\n","#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim1\n","criterion = nn.CrossEntropyLoss()\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNuilMKgepdB"},"outputs":[],"source":["model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=10)\n"]}],"metadata":{"accelerator":"GPU","colab":{"name":"(ANS)bert_example.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}